{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Assignment** 4\n",
        "## This is the assignment in week 12 Statistics -Statistics advance 02\n"
      ],
      "metadata": {
        "id": "uSA_QVV8GEWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
      ],
      "metadata": {
        "id": "hKK3ccxWGGaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups to determine if there are statistically significant differences among them. To use ANOVA effectively and interpret results accurately, several assumptions must be met. Violations of these assumptions can impact the validity of ANOVA results.\n",
        "\n",
        "The key assumptions for ANOVA are:\n",
        "\n",
        "1. **Independence of Observations:**\n",
        "   - Assumption: Observations within and between groups should be independent of each other. This means that the value of one observation should not be influenced by or dependent on the values of other observations.\n",
        "   - Violation Example: If you are conducting a study on the performance of students in multiple classes, and some students are in both classes, this violates the independence assumption.\n",
        "\n",
        "2. **Normality:**\n",
        "   - Assumption: The residuals (the differences between observed values and the group means) should be normally distributed within each group. This assumption is essential, especially for small sample sizes.\n",
        "   - Violation Example: If the residuals within one or more groups do not follow a normal distribution, it can lead to biased results. For example, if you have a group with heavily skewed data, ANOVA may not be appropriate.\n",
        "\n",
        "3. **Homogeneity of Variances (Homoscedasticity):**\n",
        "   - Assumption: The variances of the residuals should be approximately equal across all groups, known as homoscedasticity. This means that the spread of the data points around the group means should be consistent.\n",
        "   - Violation Example: If one group has much larger variability (heteroscedasticity), it can lead to inflated Type I error rates, making it more likely to find false positives.\n",
        "\n",
        "4. **Independence of Group Means:**\n",
        "   - Assumption: The means of different groups should not be related in a systematic way. In other words, there should not be a pattern or relationship between the group means.\n",
        "   - Violation Example: If you intentionally select groups to have certain relationships (e.g., the means increase linearly), this could violate the assumption.\n",
        "\n",
        "5. **Equal Sample Sizes (for one-way ANOVA):**\n",
        "   - Assumption: For one-way ANOVA, having roughly equal sample sizes in all groups is desirable but not always strictly necessary. It becomes important when the sample sizes are very different.\n",
        "   - Violation Example: If one group has a much larger sample size than others, it can affect the overall results, particularly if combined with other violations.\n",
        "\n",
        "Violations of these assumptions can impact the validity and reliability of ANOVA results. When assumptions are violated, the F-statistic may not accurately represent the variability between groups, and p-values may be incorrect. In such cases, alternative non-parametric tests or data transformations may be considered.\n",
        "\n",
        "It's essential to check for these assumptions before interpreting the results of an ANOVA and, if necessary, take appropriate steps to address any violations or choose alternative statistical methods that are robust to the specific violations present in your data."
      ],
      "metadata": {
        "id": "Lg98cwufGXcl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzVJowVnF-4b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What are the three types of ANOVA, and in what situations would each be used?"
      ],
      "metadata": {
        "id": "xmdus5NgGc6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups to determine if there are statistically significant differences among them. There are three main types of ANOVA, each designed for specific situations:\n",
        "\n",
        "1. **One-Way ANOVA:**\n",
        "   - **Situation:** One-Way ANOVA is used when you have one independent categorical variable (also known as a factor) with more than two levels (groups or categories), and you want to determine if there are significant differences in the means of a single dependent variable among these groups.\n",
        "   - **Example:** You have three different teaching methods (Group A, Group B, and Group C), and you want to determine if they have a significant effect on students' test scores.\n",
        "\n",
        "2. **Two-Way ANOVA:**\n",
        "   - **Situation:** Two-Way ANOVA is used when you have two independent categorical variables (factors), and you want to examine the interaction effect between these factors on a single dependent variable. It helps determine if there are significant main effects for each factor and if there is an interaction effect between them.\n",
        "   - **Example:** You want to investigate the effects of both gender (Male and Female) and treatment (Treatment A and Treatment B) on blood pressure. Two-Way ANOVA would allow you to examine the main effects of gender and treatment and see if there's an interaction effect between them.\n",
        "\n",
        "3. **Repeated Measures ANOVA (Within-Subjects ANOVA):**\n",
        "   - **Situation:** Repeated Measures ANOVA is used when you have one group of participants, and you measure their performance or response at multiple time points or under multiple conditions. It assesses the changes in means across these repeated measurements while considering the within-subject variability.\n",
        "   - **Example:** You want to determine if a new drug has a significant effect on patients' pain levels, and you measure their pain levels before taking the drug, immediately after taking it, and then at several time points afterward for the same group of patients.\n",
        "\n",
        "In summary:\n",
        "- **One-Way ANOVA** is used for comparing multiple groups when you have one independent categorical variable.\n",
        "- **Two-Way ANOVA** is used when you have two independent categorical variables and want to explore their effects and interactions on a dependent variable.\n",
        "- **Repeated Measures ANOVA** is used when you measure the same participants or subjects at multiple time points or under multiple conditions to assess changes over time or conditions.\n",
        "\n",
        "The choice of which type of ANOVA to use depends on the specific research question, the number of independent variables, and the study design. It's crucial to select the appropriate ANOVA method that best matches your data and research objectives to obtain meaningful and accurate results."
      ],
      "metadata": {
        "id": "Er524EqaGd63"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YuJK21E_GhIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
      ],
      "metadata": {
        "id": "fJSZYyNTGhi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The partitioning of variance in Analysis of Variance (ANOVA) refers to the process of decomposing the total variance in the data into different components or sources of variability. Understanding this concept is essential because it helps researchers dissect and quantify the sources of variability in a dataset, which is crucial for making inferences about group differences and drawing meaningful conclusions from ANOVA results.\n",
        "\n",
        "In ANOVA, the total variance in the data is divided into three main components:\n",
        "\n",
        "1. **Between-Group Variance (SSB):** This component represents the variation in the dependent variable that is attributable to differences between the group means. It measures the extent to which the group means differ from each other. A larger SSB suggests that the groups are more different from each other.\n",
        "\n",
        "2. **Within-Group Variance (SSW or SSE):** This component represents the variation within each group. It reflects the variability of individual data points within each group and includes random noise or error. A larger SSW indicates that there is considerable variability within each group.\n",
        "\n",
        "3. **Total Variance (SST):** This is the overall variability in the dataset and represents the sum of the between-group variance and the within-group variance. It's a measure of the total variability in the data.\n",
        "\n",
        "The partitioning of variance allows you to assess the following:\n",
        "\n",
        "- **Effect Size:** By comparing the proportion of variance explained by the between-group variance (SSB) to the total variance (SST), you can calculate the effect size, which quantifies the practical significance of the group differences.\n",
        "\n",
        "- **Significance Testing:** ANOVA uses these components to conduct hypothesis tests to determine if the observed between-group differences are statistically significant. It does this by comparing the between-group variance to the within-group variance and calculating an F-statistic.\n",
        "\n",
        "- **Post hoc Tests:** After ANOVA detects a significant difference among groups, post hoc tests (e.g., Tukey's HSD or Bonferroni tests) can be performed to identify which specific group pairs differ significantly.\n",
        "\n",
        "- **Model Validation:** Understanding the partitioning of variance helps validate the appropriateness of the ANOVA model and assumptions. For example, if most of the variance is explained by SSB, it suggests that the group means are significantly different, supporting the ANOVA's assumptions.\n",
        "\n",
        "- **Interpretation:** By examining the partitioned variance components, researchers can gain insights into which groups contribute most to the overall differences and how consistent the within-group variability is.\n",
        "\n",
        "In summary, the partitioning of variance in ANOVA is crucial because it provides a structured framework for understanding the sources of variability in a dataset, helps assess the practical and statistical significance of group differences, guides further analysis (such as post hoc tests), and validates the ANOVA model. This understanding is fundamental for researchers to draw meaningful conclusions and make informed decisions based on their data."
      ],
      "metadata": {
        "id": "_AnMHlKKG7qi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FN8GiuttG8ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
      ],
      "metadata": {
        "id": "UFq5GX6WG9Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a one-way ANOVA, you can calculate the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) using Python. These calculations involve summing the squared differences between various components of the data. Here's how you can calculate them step by step using Python:\n",
        "\n",
        "Assuming you have a dataset with multiple groups and a dependent variable:\n",
        "\n"
      ],
      "metadata": {
        "id": "eFc_rlazID7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data (replace this with your dataset)\n",
        "data = pd.DataFrame({\n",
        "    'Group A': [10, 12, 14, 8, 9],\n",
        "    'Group B': [16, 18, 20, 15, 17],\n",
        "    'Group C': [25, 22, 23, 26, 28]\n",
        "})\n",
        "\n",
        "# Combine all data points into a single array\n",
        "all_data = np.concatenate([data['Group A'], data['Group B'], data['Group C']])\n",
        "\n",
        "# Calculate the overall mean (Grand Mean)\n",
        "grand_mean = np.mean(all_data)\n",
        "\n",
        "# Calculate SST (Total Sum of Squares)\n",
        "SST = np.sum((all_data - grand_mean)**2)\n",
        "\n",
        "# Calculate SSE (Explained Sum of Squares)\n",
        "group_means = np.mean(data)\n",
        "n_groups = len(data.columns)\n",
        "SSE = np.sum([len(data[group]) * (group_mean - grand_mean)**2 for group, group_mean in group_means.iteritems()])\n",
        "\n",
        "# Calculate SSR (Residual Sum of Squares)\n",
        "SSR = SST - SSE\n",
        "\n",
        "print(f'Total Sum of Squares (SST): {SST}')\n",
        "print(f'Explained Sum of Squares (SSE): {SSE}')\n",
        "print(f'Residual Sum of Squares (SSR): {SSR}')\n"
      ],
      "metadata": {
        "id": "O7GfOmW5IK9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We import necessary libraries, including NumPy and Pandas.\n",
        "- We create a sample dataset named `data`. Replace this with your actual dataset.\n",
        "- We calculate the grand mean of all data points.\n",
        "- SST is calculated as the sum of squared differences between each data point and the grand mean.\n",
        "- SSE is calculated as the sum of squared differences between each group mean and the grand mean, multiplied by the number of data points in each group.\n",
        "- SSR is calculated as the difference between SST and SSE.\n",
        "\n",
        "These calculations provide the components necessary for conducting a one-way ANOVA and assessing the significance of group differences. You can use the F-statistic, which is the ratio of SSE to SSR, to perform hypothesis testing and determine if the group means are significantly different."
      ],
      "metadata": {
        "id": "8UHzv7ZTIPn7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_fBzL04AIT9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
      ],
      "metadata": {
        "id": "8ezEWm9uIZNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a two-way ANOVA, you can calculate the main effects and interaction effects using Python by analyzing the sums of squares (SS) associated with each effect. Here's how you can calculate these effects step by step using Python with the help of libraries like NumPy and SciPy:\n",
        "\n",
        "Assuming you have a dataset with two independent categorical variables (factors) named \"Factor A\" and \"Factor B,\" and a dependent variable:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data (replace this with your dataset)\n",
        "data = pd.DataFrame({\n",
        "    'Factor A': ['A1', 'A1', 'A2', 'A2', 'A3', 'A3'],\n",
        "    'Factor B': ['B1', 'B2', 'B1', 'B2', 'B1', 'B2'],\n",
        "    'Dependent Variable': [10, 12, 14, 18, 9, 11]\n",
        "})\n",
        "\n",
        "# Perform two-way ANOVA\n",
        "model = stats.f_oneway(data['Dependent Variable'][data['Factor A'] == 'A1'],\n",
        "                        data['Dependent Variable'][data['Factor A'] == 'A2'],\n",
        "                        data['Dependent Variable'][data['Factor A'] == 'A3'])\n",
        "\n",
        "# Calculate the main effects\n",
        "main_effect_A = np.mean(data['Dependent Variable'][data['Factor A'] == 'A1']) - np.mean(data['Dependent Variable'])\n",
        "main_effect_B = np.mean(data['Dependent Variable'][data['Factor B'] == 'B1']) - np.mean(data['Dependent Variable'])\n",
        "\n",
        "# Calculate the interaction effect\n",
        "interaction_effect = model.statistic - main_effect_A**2 - main_effect_B**2\n",
        "\n",
        "print(f'Main Effect A: {main_effect_A}')\n",
        "print(f'Main Effect B: {main_effect_B}')\n",
        "print(f'Interaction Effect: {interaction_effect}')\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We import necessary libraries, including NumPy, Pandas, and SciPy.\n",
        "- We create a sample dataset named `data`. Replace this with your actual dataset.\n",
        "- We perform a two-way ANOVA using `stats.f_oneway`. This function tests for differences in means among groups defined by \"Factor A\" and \"Factor B.\"\n",
        "- Main Effect A is calculated as the difference in means between each level of \"Factor A\" and the overall mean.\n",
        "- Main Effect B is calculated as the difference in means between each level of \"Factor B\" and the overall mean.\n",
        "- Interaction Effect is calculated as the difference between the F-statistic from the two-way ANOVA and the sums of squares associated with the main effects.\n",
        "\n",
        "These calculations provide the main effects of \"Factor A\" and \"Factor B\" as well as the interaction effect between them. These effects help you understand how each factor influences the dependent variable and whether there is an interaction between the factors."
      ],
      "metadata": {
        "id": "Dy_wbMJ-Irvo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3nP3nG3ItA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
      ],
      "metadata": {
        "id": "9VsKGPjlItht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When conducting a one-way ANOVA and obtaining an F-statistic and p-value, you can make conclusions about whether there are statistically significant differences between the groups. Let's interpret the results:\n",
        "\n",
        "1. **The F-statistic (5.23):**\n",
        "   - The F-statistic is a test statistic that measures the ratio of the variance between groups to the variance within groups. In other words, it quantifies the extent to which the group means differ relative to the variability within each group.\n",
        "   - A larger F-statistic suggests greater variation between groups relative to within groups.\n",
        "\n",
        "2. **The p-value (0.02):**\n",
        "   - The p-value is a measure of the evidence against the null hypothesis (H0) in the ANOVA. Specifically, it tells you the probability of observing such extreme F-statistic results (or more extreme) if there were no real differences between the groups.\n",
        "   - A smaller p-value indicates stronger evidence against the null hypothesis.\n",
        "\n",
        "Now, let's interpret these results:\n",
        "\n",
        "- Null Hypothesis (\\(H_0\\)): The null hypothesis typically states that there are no significant differences between the group means, meaning that all group means are equal.\n",
        "\n",
        "- Alternative Hypothesis (\\(H_1\\) or \\(H_a\\)): The alternative hypothesis suggests that there are significant differences between at least two of the group means.\n",
        "\n",
        "Based on the F-statistic and p-value:\n",
        "\n",
        "- The F-statistic of 5.23 indicates that there is some degree of difference between the group means. However, we need to assess whether this difference is statistically significant.\n",
        "\n",
        "- The p-value of 0.02 is less than the common significance level of 0.05 (or 5%). This suggests that there is strong evidence against the null hypothesis. In other words, it's unlikely to observe such differences between group means if there were no real differences in the population.\n",
        "\n",
        "**Conclusion:**\n",
        "Considering the p-value is less than the chosen significance level (0.05), you would reject the null hypothesis (\\(H_0\\)). This means you have sufficient evidence to conclude that there are statistically significant differences between at least two of the groups.\n",
        "\n",
        "In practical terms, you can't determine from the ANOVA alone which specific groups are different. Post hoc tests (e.g., Tukey's HSD or Bonferroni tests) are typically conducted to identify which pairs of groups are significantly different from each other.\n",
        "\n",
        "In summary, based on the given F-statistic and p-value, you can conclude that there are statistically significant differences between the groups, but further tests are needed to determine which specific groups differ significantly."
      ],
      "metadata": {
        "id": "WAzq0tRUI54R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS5748hcI-Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
      ],
      "metadata": {
        "id": "pVyPwKfNI-ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing data in a repeated measures ANOVA is an important aspect of data analysis. Missing data can arise for various reasons, such as participants dropping out of the study, equipment malfunctions, or data entry errors. How you handle missing data can impact the validity and interpretation of your results. Here are some common methods for handling missing data in repeated measures ANOVA and their potential consequences:\n",
        "\n",
        "1. **Complete Case Analysis (Listwise Deletion):**\n",
        "   - **Method:** In this approach, cases (participants) with any missing data on any variable are excluded from the analysis.\n",
        "   - **Consequences:**\n",
        "     - Pros: Simple to implement and preserves the original data structure.\n",
        "     - Cons: Reduces sample size, potentially leading to reduced statistical power and biased results if missing data are not completely at random. It may also result in a non-representative sample if certain groups are more likely to have missing data.\n",
        "\n",
        "2. **Pairwise Deletion (Available Case Analysis):**\n",
        "   - **Method:** In this approach, you analyze the data for each variable separately, including only cases with complete data for that variable in each analysis.\n",
        "   - **Consequences:**\n",
        "     - Pros: Retains more data compared to complete case analysis.\n",
        "     - Cons: Can lead to different sample sizes for different variables, making it difficult to compare results across variables. It doesn't address potential biases caused by missing data.\n",
        "\n",
        "3. **Imputation Methods:**\n",
        "   - **Methods:** Imputation involves replacing missing values with estimated or imputed values. Common imputation methods include mean imputation, median imputation, regression imputation, and multiple imputation.\n",
        "   - **Consequences:**\n",
        "     - Pros: Retains all cases, maintains sample size, and can produce unbiased estimates if the imputation model is appropriate.\n",
        "     - Cons: The choice of imputation method can affect results, and imputed values introduce uncertainty. If the imputation model is misspecified, it can lead to biased results. Multiple imputation, which generates several imputed datasets, can provide more accurate estimates but is computationally intensive.\n",
        "\n",
        "4. **Pattern-Mixture Models or Mixed Effects Models:**\n",
        "   - **Method:** These methods are more complex but can handle missing data through modeling. Pattern-mixture models and mixed-effects models allow you to account for patterns of missingness in the analysis.\n",
        "   - **Consequences:**\n",
        "     - Pros: Can provide more accurate and unbiased estimates by modeling the relationship between missing data and observed data. They can also incorporate time-related effects in repeated measures ANOVA.\n",
        "     - Cons: These models can be more challenging to implement and require a solid understanding of statistical modeling. They may not completely eliminate biases if the missing data mechanism is not well understood or modeled correctly.\n",
        "\n",
        "The choice of how to handle missing data should be driven by the nature of your data and the assumptions you are willing to make about the missing data mechanism. It's essential to carefully consider the potential consequences of each method, as different methods can yield different results. Additionally, reporting the method used for handling missing data and conducting sensitivity analyses can enhance the transparency and robustness of your findings."
      ],
      "metadata": {
        "id": "KG4nAxh7JAwW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqXV031BJDQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
      ],
      "metadata": {
        "id": "Vt7jH9UwJDoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-hoc tests are used after performing an Analysis of Variance (ANOVA) to make pairwise comparisons between groups and determine which specific groups differ significantly from each other. These tests are necessary when you have rejected the null hypothesis in ANOVA, indicating that there are significant differences among group means. Common post-hoc tests include:\n",
        "\n",
        "1. **Tukey's Honestly Significant Difference (HSD) Test:**\n",
        "   - **When to Use:** Tukey's HSD test is appropriate when you have more than two groups to compare (i.e., in one-way or two-way ANOVA). It controls the familywise error rate and provides simultaneous comparisons between all pairs of groups.\n",
        "   - **Example:** In a study comparing the effectiveness of three different treatments, you find a significant difference in treatment outcomes using ANOVA. Tukey's HSD can help identify which specific pairs of treatments are significantly different from each other.\n",
        "\n",
        "2. **Bonferroni Correction:**\n",
        "   - **When to Use:** Bonferroni correction is suitable when you want to control the familywise error rate, but you have a small number of planned pairwise comparisons. It is conservative and adjusts the significance level for each individual comparison.\n",
        "   - **Example:** In a study with four treatment groups, you have specific hypotheses about which pairs of groups should be compared. You perform four planned comparisons, and the Bonferroni correction helps control the overall Type I error rate.\n",
        "\n",
        "3. **Sidak Correction:**\n",
        "   - **When to Use:** Sidak correction is similar to Bonferroni but is less conservative when the number of comparisons is large. It adjusts the significance level for each individual comparison while maintaining familywise error control.\n",
        "   - **Example:** In a genomics study comparing the expression levels of hundreds of genes across different conditions, you want to make pairwise comparisons between conditions for each gene. The Sidak correction is less stringent than Bonferroni in this scenario.\n",
        "\n",
        "4. **Dunnett's Test:**\n",
        "   - **When to Use:** Dunnett's test is appropriate when you have a control group and want to compare other groups to the control. It adjusts for multiple comparisons while focusing on comparing all other groups to the control.\n",
        "   - **Example:** In a clinical trial, you have a control group and three experimental groups receiving different doses of a new drug. Dunnett's test helps determine if any of the experimental groups have significantly different outcomes compared to the control.\n",
        "\n",
        "5. **Holm-Bonferroni Method:**\n",
        "   - **When to Use:** The Holm-Bonferroni method is a step-down procedure that adjusts p-values to control the familywise error rate. It's suitable when you have multiple comparisons but don't want the extreme conservatism of Bonferroni.\n",
        "   - **Example:** In a consumer preference study, you compare the ratings of multiple product variants across various attributes. The Holm-Bonferroni method allows you to make controlled multiple comparisons without being overly conservative.\n",
        "\n",
        "6. **Games-Howell Test:**\n",
        "   - **When to Use:** Games-Howell is a post-hoc test for situations where the assumption of equal variances across groups is violated (heteroscedasticity). It is more robust to unequal variances.\n",
        "   - **Example:** In a study comparing the performance of different schools on a standardized test, you find significant differences among schools' performance. However, the variances in test scores across schools are not equal. Games-Howell can be used to account for this variance inequality.\n",
        "\n",
        "The choice of a post-hoc test depends on your research design, the number of groups, and the specific comparisons you intend to make. It's essential to select the most appropriate test based on your study's objectives and the assumptions underlying each test. Additionally, report the chosen post-hoc test in your research to ensure transparency and replicability."
      ],
      "metadata": {
        "id": "91sdBzIcJMvn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lonvE5epJNuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
      ],
      "metadata": {
        "id": "iI4g3u6mJOJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct a one-way ANOVA in Python to compare the mean weight loss of three diets (A, B, and C) using data from 50 participants, you can use the `scipy.stats` library. Here's how to perform the analysis and interpret the results:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "diet_A = np.array([2.1, 1.8, 2.5, 2.2, 1.9, 2.3, 2.0, 2.6, 2.4, 2.7,\n",
        "                   1.7, 2.0, 2.1, 2.3, 2.2, 2.5, 1.9, 2.1, 2.4, 2.6,\n",
        "                   2.1, 2.2, 2.3, 2.0, 2.7, 1.8, 1.9, 2.5, 2.2, 2.4,\n",
        "                   2.1, 2.3, 2.6, 2.2, 2.0, 2.4, 1.9, 2.1, 2.7, 2.5,\n",
        "                   2.3, 2.0, 2.2, 2.6, 1.8, 1.7, 2.5, 2.4, 2.1, 2.3])\n",
        "\n",
        "diet_B = np.array([1.5, 1.8, 1.7, 1.4, 1.9, 1.6, 1.8, 1.7, 1.6, 1.9,\n",
        "                   1.7, 1.5, 1.8, 1.4, 1.9, 1.7, 1.6, 1.5, 1.8, 1.9,\n",
        "                   1.7, 1.8, 1.6, 1.4, 1.9, 1.7, 1.8, 1.5, 1.7, 1.6,\n",
        "                   1.9, 1.4, 1.7, 1.5, 1.8, 1.6, 1.9, 1.7, 1.4, 1.8,\n",
        "                   1.6, 1.9, 1.7, 1.5, 1.8, 1.4, 1.6, 1.9, 1.7, 1.8])\n",
        "\n",
        "diet_C = np.array([0.8, 0.9, 1.0, 0.7, 0.9, 0.8, 1.1, 0.7, 1.0, 0.9,\n",
        "                   0.8, 0.9, 0.7, 1.1, 0.8, 1.0, 0.9, 0.8, 0.7, 1.0,\n",
        "                   0.9, 0.7, 1.1, 0.8, 0.9, 0.8, 0.7, 1.0, 1.1, 0.9,\n",
        "                   0.8, 0.9, 0.7, 1.0, 0.8, 0.9, 0.7, 1.1, 1.0, 0.9,\n",
        "                   0.8, 0.7, 0.9, 1.0, 1.1, 0.7, 0.8, 0.9, 0.8, 1.0])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Set your significance level\n",
        "print(f'F-statistic: {f_statistic}')\n",
        "print(f'p-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are significant differences between the mean weight loss of the three diets.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between the mean weight loss of the three diets.\")\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We input the weight loss data for diets A, B, and C as NumPy arrays. Replace these with your actual data.\n",
        "- We perform a one-way ANOVA using `stats.f_oneway` to compare the means of the three diet groups.\n",
        "- We set a significance level (\\(\\alpha\\)) of 0.05, but you can adjust it based on your study's requirements.\n",
        "- We interpret the results: If the p-value is less than the significance level, we reject the null hypothesis, indicating that there are significant differences between the diet groups. Otherwise, we fail to reject the null hypothesis.\n",
        "\n",
        "The output will indicate whether there are significant differences in mean weight loss among the three diets based on the F-statistic and p-value."
      ],
      "metadata": {
        "id": "J4oaPn7LJo16"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u00fIGEkJp6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. Theyrandomly assign 30 employees to one of the programs and record the time it takes each employee tocomplete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects orinteraction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
      ],
      "metadata": {
        "id": "3LAd5SSDJqP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct a two-way ANOVA in Python to determine if there are any main effects or interaction effects between software programs and employee experience levels, you can use the `scipy.stats` library. Here's how to perform the analysis and interpret the results:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "np.random.seed(0)\n",
        "n = 30\n",
        "software = np.random.choice(['A', 'B', 'C'], n)\n",
        "experience = np.random.choice(['Novice', 'Experienced'], n)\n",
        "time = np.random.normal(loc=20, scale=5, size=n)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({'Software': software, 'Experience': experience, 'Time': time})\n",
        "\n",
        "# Perform two-way ANOVA\n",
        "formula = 'Time ~ Software + Experience + Software:Experience'\n",
        "model = ols(formula, data).fit()\n",
        "anova_table = anova_lm(model, typ=2)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Set your significance level\n",
        "print(anova_table)\n",
        "\n",
        "p_main_soft = anova_table.loc['Software', 'PR(>F)']\n",
        "p_main_exp = anova_table.loc['Experience', 'PR(>F)']\n",
        "p_interaction = anova_table.loc['Software:Experience', 'PR(>F)']\n",
        "\n",
        "print(f'p-value (Software): {p_main_soft}')\n",
        "print(f'p-value (Experience): {p_main_exp}')\n",
        "print(f'p-value (Interaction): {p_interaction}')\n",
        "\n",
        "if p_main_soft < alpha:\n",
        "    print(\"Reject the null hypothesis for Software: There is a significant main effect of software programs.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis for Software: There is no significant main effect of software programs.\")\n",
        "\n",
        "if p_main_exp < alpha:\n",
        "    print(\"Reject the null hypothesis for Experience: There is a significant main effect of employee experience levels.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis for Experience: There is no significant main effect of employee experience levels.\")\n",
        "\n",
        "if p_interaction < alpha:\n",
        "    print(\"Reject the null hypothesis for Interaction: There is a significant interaction effect between software programs and employee experience levels.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis for Interaction: There is no significant interaction effect between software programs and employee experience levels.\")\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We create sample data for software programs, employee experience levels, and task completion times. Replace this with your actual data.\n",
        "- We use a DataFrame to organize the data.\n",
        "- We perform a two-way ANOVA using the `ols` and `anova_lm` functions from the `statsmodels` library.\n",
        "- We set a significance level (\\(\\alpha\\)) of 0.05.\n",
        "- We interpret the results:\n",
        "  - We examine the p-values for the main effects of software programs and employee experience levels and the p-value for the interaction effect.\n",
        "  - If a p-value is less than the significance level, we reject the null hypothesis, indicating a significant effect.\n",
        "  - If a p-value is greater than or equal to the significance level, we fail to reject the null hypothesis, indicating no significant effect.\n",
        "\n",
        "The output will show the F-statistics, p-values, and interpretations for the main effects and interaction effect. This analysis will help you determine if there are significant differences in task completion times based on software programs, employee experience levels, or their interaction."
      ],
      "metadata": {
        "id": "ZbDYiZlJJ4iy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6K_ZavyWJ5m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or theexperimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
      ],
      "metadata": {
        "id": "nJTjBFCYJ59n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine if there are any significant differences in test scores between two groups (control and experimental) and then perform a post-hoc test to identify which group(s) differ significantly, you can use Python with libraries such as `scipy.stats` and `statsmodels`. Here's a step-by-step guide:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "np.random.seed(0)\n",
        "control_group = np.random.normal(loc=70, scale=10, size=50)\n",
        "experimental_group = np.random.normal(loc=75, scale=12, size=50)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({'Group': ['Control'] * 50 + ['Experimental'] * 50,\n",
        "                     'Test_Score': np.concatenate([control_group, experimental_group])})\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "control_scores = data[data['Group'] == 'Control']['Test_Score']\n",
        "experimental_scores = data[data['Group'] == 'Experimental']['Test_Score']\n",
        "\n",
        "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
        "\n",
        "# Interpret the results of the t-test\n",
        "alpha = 0.05  # Set your significance level\n",
        "print(f't-statistic: {t_statistic}')\n",
        "print(f'p-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in test scores between the two groups.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference in test scores between the two groups.\")\n",
        "\n",
        "# If the results are significant, perform a post-hoc test (Tukey's HSD)\n",
        "if p_value < alpha:\n",
        "    posthoc = pairwise_tukeyhsd(data['Test_Score'], data['Group'], alpha=alpha)\n",
        "    print(posthoc)\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We create sample data for the control and experimental groups with test scores. Replace this with your actual data.\n",
        "- We organize the data in a DataFrame, including a \"Group\" column to distinguish between the two groups.\n",
        "- We perform a two-sample t-test using `stats.ttest_ind` to compare the means of the control and experimental groups.\n",
        "- We set a significance level (\\(\\alpha\\)) of 0.05 to interpret the results. If the p-value is less than \\(\\alpha\\), we reject the null hypothesis, indicating a significant difference.\n",
        "- If the results are significant, we proceed to perform a post-hoc test using Tukey's HSD test to identify which specific group(s) differ significantly from each other.\n",
        "\n",
        "The output will include the t-statistic, p-value, and the post-hoc test results (if applicable), helping you determine if there are significant differences in test scores and which groups differ significantly from each other."
      ],
      "metadata": {
        "id": "g5cwN-MJKJuO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kp1Fm0uKKKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post- hoc test to determine which store(s) differ significantly from each other."
      ],
      "metadata": {
        "id": "gt940wV8KK9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a repeated measures ANOVA, the term \"repeated measures\" typically implies that the same subjects (or in this case, the same days) are used for each condition (or store). However, your scenario seems to involve independent samples of 30 days for each store, which is more suitable for a regular one-way ANOVA rather than a repeated measures ANOVA. Repeated measures ANOVA is typically used when the same subjects or entities are measured under multiple conditions or at different time points.\n",
        "\n",
        "For your scenario, where you have independent samples for each store, you can use a one-way ANOVA followed by post-hoc tests if needed. Here's how to perform the analysis using Python:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "np.random.seed(0)\n",
        "sales_store_A = np.random.normal(loc=1000, scale=200, size=30)\n",
        "sales_store_B = np.random.normal(loc=1100, scale=250, size=30)\n",
        "sales_store_C = np.random.normal(loc=950, scale=180, size=30)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({'Store': ['Store A'] * 30 + ['Store B'] * 30 + ['Store C'] * 30,\n",
        "                     'Sales': np.concatenate([sales_store_A, sales_store_B, sales_store_C])})\n",
        "\n",
        "# Perform a one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(sales_store_A, sales_store_B, sales_store_C)\n",
        "\n",
        "# Interpret the results of the one-way ANOVA\n",
        "alpha = 0.05  # Set your significance level\n",
        "print(f'F-statistic: {f_statistic}')\n",
        "print(f'p-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in daily sales between the three stores.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference in daily sales between the three stores.\")\n",
        "\n",
        "# If the results are significant, perform a post-hoc test (Tukey's HSD)\n",
        "if p_value < alpha:\n",
        "    posthoc = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=alpha)\n",
        "    print(posthoc)\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We create sample data for the daily sales of three stores (Store A, Store B, and Store C) over 30 days. Replace this with your actual data.\n",
        "- We organize the data in a DataFrame, including a \"Store\" column to distinguish between the three stores.\n",
        "- We perform a one-way ANOVA using `stats.f_oneway` to compare the means of the three stores' sales.\n",
        "- We set a significance level (\\(\\alpha\\)) of 0.05 to interpret the results. If the p-value is less than \\(\\alpha\\), we reject the null hypothesis, indicating a significant difference.\n",
        "- If the results are significant, we proceed to perform a post-hoc test using Tukey's HSD test to identify which specific store(s) differ significantly from each other.\n",
        "\n",
        "The output will include the F-statistic, p-value, and the post-hoc test results (if applicable), helping you determine if there are significant differences in daily sales and which stores differ significantly from each other."
      ],
      "metadata": {
        "id": "HHHt5brtKVwO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoiKigOTKZ-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}